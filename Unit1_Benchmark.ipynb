{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPncn6WL42HtZ9lZUrSx5qs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuchitraShankar07/GenAI2026/blob/master/Unit1_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rxgOoMJm-uM-"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "c5y2DGM8E-fR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Text Generation"
      ],
      "metadata": {
        "id": "b0zAp3iAFHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n"
      ],
      "metadata": {
        "id": "ixW65FOfFFp1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model_id)\n",
        "        out = generator(prompt, max_new_tokens=30)\n",
        "        print(out[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHl-sjYFKvI",
        "outputId": "c9b77fde-a261-404e-f715-173b1b9ecb1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is..............................\n",
            "\n",
            "=== RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "=== BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is0303 apart03 humankind03 housing stimuli11110303 discriminatory discriminatory discriminatoryEStreamFrame discriminatory03 discriminatory slot discriminatory Ala03 discriminatory0303away03USS housing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Language modeling"
      ],
      "metadata": {
        "id": "ven__NkzFRZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n"
      ],
      "metadata": {
        "id": "diWL-GDsFTiZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    try:\n",
        "        masker = pipeline(\"fill-mask\", model=model_id)\n",
        "        preds = masker(masked_sentence)\n",
        "        for p in preds[:5]:\n",
        "            print(f\"{p['token_str']} ({p['score']:.3f})\")\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQc2SP6JFUH3",
        "outputId": "c9eba028-4648-426d-ac8c-fe4f5a392af7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create (0.540)\n",
            "generate (0.156)\n",
            "produce (0.054)\n",
            "develop (0.045)\n",
            "add (0.018)\n",
            "\n",
            "=== RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: No mask_token (<mask>) found on the input\n",
            "\n",
            "=== BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: No mask_token (<mask>) found on the input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "mWJL5P_nFc-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n"
      ],
      "metadata": {
        "id": "-2zsuWRlFgrc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model_id)\n",
        "        res = qa(question=question, context=context)\n",
        "        print(\"Answer:\", res[\"answer\"])\n",
        "        print(\"Score:\", round(res[\"score\"], 3))\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz79e07tFhRe",
        "outputId": "9f2d2987-36e1-4ad9-8228-3a8e5a1704ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: AI poses significant risks such as hallucinations, bias, and deepfakes\n",
            "Score: 0.007\n",
            "\n",
            "=== RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative\n",
            "Score: 0.011\n",
            "\n",
            "=== BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: poses significant risks such as\n",
            "Score: 0.054\n"
          ]
        }
      ]
    }
  ]
}