{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNEvaw1jclFzTRp/DmrGAyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuchitraShankar07/GenAI2026/blob/master/Unit1_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rxgOoMJm-uM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30279eaa-8685-4955-b1c3-98940f290fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "c5y2DGM8E-fR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Text Generation"
      ],
      "metadata": {
        "id": "b0zAp3iAFHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n"
      ],
      "metadata": {
        "id": "ixW65FOfFFp1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model_id)\n",
        "        out = generator(prompt, max_new_tokens=30)\n",
        "        print(out[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHl-sjYFKvI",
        "outputId": "8364f68e-68a2-472f-9a5c-72de989ef048"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is..............................\n",
            "\n",
            "=== RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "=== BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is0303 apart03 humankind03 housing stimuli11110303 discriminatory discriminatory discriminatoryEStreamFrame discriminatory03 discriminatory slot discriminatory Ala03 discriminatory0303away03USS housing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Language modeling"
      ],
      "metadata": {
        "id": "ven__NkzFRZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "print(f\"\\n === BERT ===\")\n",
        "try:\n",
        "    masker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "    preds = masker(masked_sentence)\n",
        "    for p in preds[:5]:\n",
        "        print(f\"{p['token_str']} ({p['score']:.3f})\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)"
      ],
      "metadata": {
        "id": "diWL-GDsFTiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316f8fa8-422b-4b64-f6fc-3a8935a1c99e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " === BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create (0.540)\n",
            "generate (0.156)\n",
            "produce (0.054)\n",
            "develop (0.045)\n",
            "add (0.018)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "print(f\"\\n === RoBERTa ===\")\n",
        "try:\n",
        "    masker = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "    preds = masker(masked_sentence)\n",
        "    for p in preds[:5]:\n",
        "        print(f\"{p['token_str']} ({p['score']:.3f})\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQc2SP6JFUH3",
        "outputId": "f566b750-e0f6-42b4-be0d-bd006ebef081"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " === RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate (0.371)\n",
            " create (0.368)\n",
            " discover (0.084)\n",
            " find (0.021)\n",
            " provide (0.017)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "print(f\"\\n === BART ===\")\n",
        "try:\n",
        "    masker = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "    preds = masker(masked_sentence)\n",
        "    for p in preds[:5]:\n",
        "        print(f\"{p['token_str']} ({p['score']:.3f})\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke0C-ciGJfQB",
        "outputId": "492da124-4b5d-4d2f-ebf2-f9bedeb4c65e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " === BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create (0.075)\n",
            " help (0.066)\n",
            " provide (0.061)\n",
            " enable (0.036)\n",
            " improve (0.033)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "mWJL5P_nFc-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n"
      ],
      "metadata": {
        "id": "-2zsuWRlFgrc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model_id)\n",
        "        res = qa(question=question, context=context)\n",
        "        print(\"Answer:\", res[\"answer\"])\n",
        "        print(\"Score:\", round(res[\"score\"], 3))\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz79e07tFhRe",
        "outputId": "256f5c55-befe-4a13-ddaa-43b6db7d2bb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BERT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: AI poses significant risks such as hallucinations, bias, and deepfakes\n",
            "Score: 0.007\n",
            "\n",
            "=== RoBERTa ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative\n",
            "Score: 0.011\n",
            "\n",
            "=== BART ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: poses significant risks such as\n",
            "Score: 0.054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HuJmOOj6HYb8"
      }
    }
  ]
}